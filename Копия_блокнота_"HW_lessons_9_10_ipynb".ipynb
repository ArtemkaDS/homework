{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"HW_lessons_9-10.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemkaDS/homework/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22HW_lessons_9_10_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgxdX4L0dSw7"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piodEhR7orUF"
      },
      "source": [
        "Ноутбук в колаб: https://colab.research.google.com/drive/1d-vvpJW8IWSPeodFnfaEVaZIQrys8uU0?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3_T-pcpnme8"
      },
      "source": [
        "**Дедлайн: 01.01.2021, 23:59**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RmFtRDn5vy"
      },
      "source": [
        "Формат отчетности - jupyter notebook. Однако вычислять производные не обязательно в Markdown. Если вычисляете вручную, то дополнительно с ноутбуком, отправляйте pdf-файл с расписанным решением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDx6dKzeEhp"
      },
      "source": [
        "### Пример реализации градиентного спуска: https://github.com/ddvika/Data-Science-School-2020/blob/main/lecture_9/gradient_methods.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-G1btbwoStu"
      },
      "source": [
        "# Задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1XambpmoeIj"
      },
      "source": [
        "Перед выполнением ДЗ посмотрите на ноутбук, прикрепленный по ссылке выше. Там вы найдете реализацию градиентного спуска с постоянным и дробным шагом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnSTXgrueAp0"
      },
      "source": [
        "### Задание 1. [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XmQ1AO9f1Qz"
      },
      "source": [
        "Релизуйте градиентный спуск с постоянным шагом и с дробным шагом для функции\n",
        "$$\n",
        "y = x_{1}^{2}+5 x_{2}^{2}\n",
        "$$\n",
        "\n",
        "в произвольно выбранной Вами точке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXdL_PDgYpS"
      },
      "source": [
        "Поэксперементируйте с разными значениями шага (скорости обучения), попробуйте хотя бы по 2-3 разных значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbvMBsdgOwV"
      },
      "source": [
        "### Задание 2. [3 points]\n",
        "\n",
        "Для функции из предыдущего задания реализуйте градиентный спуск, в котором значение шага (скорости обучения) будет изменяться по формуле циклического косинусного ожига. ( в англ. литературе - cosine annealing learning rate или cosine decay lr). \n",
        "\n",
        "Доп. литература:\n",
        "- Циклический косинусный отжиг https://habr.com/ru/post/332534/\n",
        "\n",
        "- Пример colise decay в библиотеке Pytorch:\n",
        "https://www.programmersought.com/article/12164650026/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Non6JiiVoJ"
      },
      "source": [
        "### Задание 3. [0.75 point]\n",
        "Проверьте работу Вашего градиентного спуска с косинусным отжигом на произвольной функции ( полином должен быть не меньше 3-ьего порядка и задан в пространстве не меньше $R^3$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpONErtji81D"
      },
      "source": [
        "### Задание 4.\n",
        "Случайная величина X задана следующей функцией распределения:\n",
        "$$\n",
        "f(x)=\\left\\{\\begin{array}{r}\n",
        "0 \\text { npu } x \\leq \\pi \\\\\n",
        "-\\cos x \\text { npu } \\pi<x \\leq \\frac{3}{2} \\pi \\\\\n",
        "\\text { 0 npu } x>\\frac{3}{2} \\pi\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "1. Постройте данную функцию распределения при помощи библиотеки seaborn **[0.25 point]**\n",
        "2. Найдите плотность вероятности. **[1 point]**\n",
        "3. Постройте график полученной плотности вероятности **[0.25 point]**\n",
        "\n",
        "4. Определить вероятность попадания случайной величины X в интервал $\\left[\\pi, \\frac{5}{4} \\pi\\right]$ **[0.75 point]**\n",
        "\n",
        "5. Найти математическое ожидание и дисперсию случайной величины X . **[0.75 point]**\n",
        "\n",
        "Так как мы не проходили интегрирование, то в 4 и 5 пунктах можете использовать\n",
        "wolfram alpha (https://www.wolframalpha.com) для интегрирования. Однако 2ой пункт задания (на нахождение производной) должен быть расписан!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LrqILMHkgTd"
      },
      "source": [
        "### Задание 5 [1.5 point]\n",
        "\n",
        " Случайная величина Х задана функцией распределения F(x).\n",
        "\n",
        " $$F(x)=\\left\\{\\begin{array}{c}0, x \\leq 1 \\\\ x-1,1<x \\leq 2 \\\\ 1, x>2\\end{array}\\right.$$\n",
        "\n",
        " 1. Является ли случайная величина Х непрерывной?\n",
        "\n",
        " 2. имеет ли случайная величина Х плотность вероятности f(X)? Если имеет, найти ее. \n",
        " 3. постройте графики f(X) и F(X), если такое возможно.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQy9iYWmGD5"
      },
      "source": [
        "### Задание 6\n",
        "\n",
        "Рассмотрим несбалансированный набор данных с соотношением меньшего класса к большему 1: 100, где 100 экземпляров принадлежит меньшему классу, а 10 000 большему.\n",
        "\n",
        "Модель ML делает прогнозы и предсказывает 120 примеров как принадлежащих к классу меньшинства, 90 из которых верны, а 30 - неверны.\n",
        "\n",
        "Найти:\n",
        "\n",
        "- Precision **[0.5 point]**\n",
        "- Recall **[0.5 point]**\n",
        "- $F_1$ метрику **[0.5 point]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnkSjI5tdWJe"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-R8lHARrZEF"
      },
      "source": [
        "$\r\n",
        "y = x_{1}^{2}+5 x_{2}^{2}\r\n",
        "$\r\n",
        "\r\n",
        "$\r\n",
        "y' = 2x_1 + 10x_2\r\n",
        "$\r\n",
        "\r\n",
        "Точка $x_0 = (2,5)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mye_Q-jgdP0g"
      },
      "source": [
        "#1\r\n",
        "def f(x):\r\n",
        "  return x[0]**2 + 5*x[1]**2\r\n",
        "def grad_f(x):\r\n",
        "  return np.array([2*x[0], 10*x[1]])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLpTJxUSuKhY"
      },
      "source": [
        "def grad_descent_const_step(x = np.array([2, 5]), alpha = 0.5, epsilon = 0.05):\r\n",
        "  grad = grad_f(x)\r\n",
        "  n = 0\r\n",
        "  check = 0\r\n",
        "  while (np.linalg.norm(grad) > epsilon) or (check < 3):\r\n",
        "    x = x - alpha*grad\r\n",
        "    grad = grad_f(x)\r\n",
        "    n += 1\r\n",
        "    if (np.linalg.norm(grad) <= epsilon):\r\n",
        "      check += 1\r\n",
        "  print(f\"Constant step gradient descent performed {n} steps\")\r\n",
        "  print(f\"Point with coordinates x1 = {x[0]}, x2 = {x[1]}\")\r\n",
        "  return x"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYBVnRx1zSl0",
        "outputId": "dc93bc1a-f9d3-4ded-8434-737c75fbfedb"
      },
      "source": [
        "a1 = grad_descent_const_step(alpha = 0.05)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constant step gradient descent performed 44 steps\n",
            "Point with coordinates x1 = 0.01939547459575047, x2 = 2.8421709430404007e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb0MphY18mNb",
        "outputId": "b0b8edf8-34f9-4351-e689-d418e177b997"
      },
      "source": [
        "a2 = grad_descent_const_step(alpha = 0.15)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constant step gradient descent performed 15 steps\n",
            "Point with coordinates x1 = 0.009495123019886002, x2 = -0.000152587890625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cnIEqm58pLp",
        "outputId": "bf0a6a09-020e-480c-804e-de2d703a6de6"
      },
      "source": [
        "a3 = grad_descent_const_step(alpha=0.01, epsilon=0.02)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constant step gradient descent performed 265 steps\n",
            "Point with coordinates x1 = 0.00946106560920773, x2 = 3.743130820571718e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZq_1Xl442Vq"
      },
      "source": [
        "def grad_descent_step_splitting(x = np.array([2, 5]), alpha = 0.001, epsilon = 0.01, ksi = 0.2, lambda_d = 0.15):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    n_alpha = 0\r\n",
        "    alpha_k = alpha\r\n",
        "    x_k0 = x\r\n",
        "    check = 0\r\n",
        "    while np.linalg.norm(grad) > epsilon or check < 3:\r\n",
        "        grad = grad_f(x_k0)\r\n",
        "        x_k1 = x_k0 - alpha_k*grad\r\n",
        "        while f(x_k1) - f(x_k0) > - alpha_k * ksi * (np.linalg.norm(grad)**2):\r\n",
        "            alpha_k *= lambda_d\r\n",
        "            x_k1 = x_k0 - alpha_k*grad\r\n",
        "            n_alpha+=1\r\n",
        "        x_k0 = x_k0 - alpha_k*grad\r\n",
        "        alpha_k = alpha\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    x = x_k0\r\n",
        "    print(f\"Splitting step gradient descent performed {n} steps\")\r\n",
        "    print(f\"{n_alpha} iterations of step splitting performed\")\r\n",
        "    print(f\"Point with coordinates x1 = {x[0]}, x2 = {x[1]}\")\r\n",
        "    return x"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R00TpazI7uw6",
        "outputId": "9cce373a-7994-401a-a332-03351021f504"
      },
      "source": [
        "b1 = grad_descent_step_splitting(alpha = 0.2)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting step gradient descent performed 38 steps\n",
            "26 iterations of step splitting performed\n",
            "Point with coordinates x1 = 0.0008713029688942997, x2 = 0.0004693740168823884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9h0issS9JxO",
        "outputId": "afdf16d4-dc22-465a-de5b-d47469cbeb9a"
      },
      "source": [
        "b2 = grad_descent_step_splitting(alpha = 0.3)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting step gradient descent performed 29 steps\n",
            "23 iterations of step splitting performed\n",
            "Point with coordinates x1 = 0.0009361425102613224, x2 = 0.000341579530050368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCGoBAVS9OD8",
        "outputId": "9b2a4460-81e4-47f3-c4b6-793c25826cb2"
      },
      "source": [
        "b3 = grad_descent_step_splitting(alpha = 0.6)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting step gradient descent performed 13 steps\n",
            "8 iterations of step splitting performed\n",
            "Point with coordinates x1 = -0.00013082501495391842, x2 = -0.00015625000000000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8kHeZHf8Lcu"
      },
      "source": [
        "#2\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i3lm79DQ8q4"
      },
      "source": [
        "class CosineAnnealingLR(_LRScheduler):\r\n",
        "    \"\"\"\r\n",
        "        optimizer(custom): optimizer.\r\n",
        "        first_cycle_steps(int): First cycle step\r\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\r\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\r\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\r\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\r\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\r\n",
        "        last_epoch(int): The index of last epoch. Default: -1.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 optimizer : torch.optim.Optimizer,\r\n",
        "                 first_cycle_steps : int,\r\n",
        "                 cycle_mult : float = 1.,\r\n",
        "                 max_lr : float = 0.1,\r\n",
        "                 min_lr : float = 0.001,\r\n",
        "                 warmup_steps : int = 0,\r\n",
        "                 gamma : float = 1.,\r\n",
        "                 last_epoch : int = -1\r\n",
        "        ):\r\n",
        "        assert warmup_steps < first_cycle_steps\r\n",
        "        \r\n",
        "        self.first_cycle_steps = first_cycle_steps\r\n",
        "        self.cycle_mult = cycle_mult\r\n",
        "        self.base_max_lr = max_lr\r\n",
        "        self.max_lr = max_lr \r\n",
        "        self.min_lr = min_lr\r\n",
        "        self.warmup_steps = warmup_steps\r\n",
        "        self.gamma = gamma\r\n",
        "        \r\n",
        "        self.cur_cycle_steps = first_cycle_steps\r\n",
        "        self.cycle = 0\r\n",
        "        self.step_in_cycle = last_epoch\r\n",
        "        \r\n",
        "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\r\n",
        "        \r\n",
        "        # set learning rate min_lr\r\n",
        "        self.init_lr()\r\n",
        "    \r\n",
        "    def init_lr(self):\r\n",
        "        self.base_lrs = []\r\n",
        "        for param_group in self.optimizer.param_groups:\r\n",
        "            param_group['lr'] = self.min_lr\r\n",
        "            self.base_lrs.append(self.min_lr)\r\n",
        "    \r\n",
        "    def get_lr(self):\r\n",
        "        if self.step_in_cycle == -1:\r\n",
        "            return self.base_lrs\r\n",
        "        elif self.step_in_cycle < self.warmup_steps:\r\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\r\n",
        "        else:\r\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\r\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\r\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\r\n",
        "                    for base_lr in self.base_lrs]\r\n",
        "\r\n",
        "    def step(self, epoch=None):\r\n",
        "        if epoch is None:\r\n",
        "            epoch = self.last_epoch + 1\r\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\r\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\r\n",
        "                self.cycle += 1\r\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\r\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\r\n",
        "        else:\r\n",
        "            if epoch >= self.first_cycle_steps:\r\n",
        "                if self.cycle_mult == 1.:\r\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\r\n",
        "                    self.cycle = epoch // self.first_cycle_steps\r\n",
        "                else:\r\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\r\n",
        "                    self.cycle = n\r\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\r\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\r\n",
        "            else:\r\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\r\n",
        "                self.step_in_cycle = epoch\r\n",
        "                \r\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\r\n",
        "        self.last_epoch = math.floor(epoch)\r\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPNgdgZ0hEyT",
        "outputId": "2c6c23d8-bb2d-4b24-fb38-4ddac5359ed9"
      },
      "source": [
        "#draft\r\n",
        "model = nn.Linear(2, 5)\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\r\n",
        "steps = 5\r\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\r\n",
        "\r\n",
        "for epoch in range(10):\r\n",
        "    for idx in range(steps):\r\n",
        "        scheduler.step()\r\n",
        "        print(scheduler.get_lr())\r\n",
        "    \r\n",
        "    print('Reset scheduler')\r\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n",
            "[0.8181356214843422]\n",
            "[0.47360679774997894]\n",
            "[0.1823725421878943]\n",
            "[0.026393202250021047]\n",
            "[0.0]\n",
            "Reset scheduler\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJ5gvTSqqtA"
      },
      "source": [
        "#3\r\n",
        "def grad_descent_cosine_annealing(x = np.array([2, 5]), alpha = 0.001, epsilon = 0.01, ksi = 0.2, lambda_d = 0.15):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    n_alpha = 0\r\n",
        "    alpha_k = alpha\r\n",
        "    x_k0 = x\r\n",
        "    check = 0\r\n",
        "    while np.linalg.norm(grad) > epsilon or check < 3:\r\n",
        "        grad = grad_f(x_k0)\r\n",
        "        x_k1 = x_k0 - alpha_k*grad\r\n",
        "        while f(x_k1) - f(x_k0) > - alpha_k * ksi * (np.linalg.norm(grad)**2):\r\n",
        "            alpha_k *= lambda_d\r\n",
        "            x_k1 = x_k0 - alpha_k*grad\r\n",
        "            n_alpha+=1\r\n",
        "        x_k0 = x_k0 - alpha_k*grad\r\n",
        "        alpha_k = alpha\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    x = x_k0\r\n",
        "    print(f\"Splitting step gradient descent performed {n} steps\")\r\n",
        "    print(f\"{n_alpha} iterations of step splitting performed\")\r\n",
        "    print(f\"Point with coordinates x1 = {x[0]}, x2 = {x[1]}\")\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTqp-Dd8tqvq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBt9TFPvtrgN"
      },
      "source": [
        "Задание 5.\r\n",
        "1. Нет\r\n",
        "2. Да. \r\n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8tcLW-vt7aS"
      },
      "source": [
        "Не хватило времени :((("
      ]
    }
  ]
}